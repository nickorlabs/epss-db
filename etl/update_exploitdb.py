import os
import csv
import requests
import psycopg2
from io import StringIO
import logging
from cve_utils import ensure_cve_exists
from etl_utils import safe_execute, safe_executemany, RunMetrics, dry_run_notice

# Set up logging based on environment
ENV = os.environ.get('ENV', 'development').lower()
LOG_LEVEL = logging.INFO if ENV == 'production' else logging.DEBUG
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')

EXPLOITDB_DATA_DIR = os.path.join(os.path.dirname(__file__), 'exploitdb-data')
if not os.path.exists(EXPLOITDB_DATA_DIR):
    os.makedirs(EXPLOITDB_DATA_DIR)
EXPLOITDB_CSV_LOCAL = os.path.join(EXPLOITDB_DATA_DIR, "files_exploits.csv")
EXPLOITS_CSV = os.path.join(EXPLOITDB_DATA_DIR, "exploits.csv")
def read_secret(secret_path, default):
    try:
        with open(secret_path, 'r') as f:
            return f.read().strip()
    except Exception:
        return default

PG_CONFIG = {
    'host': os.environ.get('PGHOST', 'db'),
    'user': read_secret('/run/secrets/pg_user', os.environ.get('PGUSER', 'postgres')),
    'password': read_secret('/run/secrets/pg_password', os.environ.get('PGPASSWORD', 'postgres')),
    'dbname': os.environ.get('PGDATABASE', 'epssdb'),
}

EXPLOITDB_CSV_URL = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"


def download_exploitdb_csv():
    logging.info(f"Downloading ExploitDB CSV from {EXPLOITDB_CSV_URL} ...")
    resp = requests.get(EXPLOITDB_CSV_URL)
    resp.raise_for_status()
    with open(EXPLOITDB_CSV_LOCAL, "w", encoding="utf-8") as f:
        f.write(resp.text)
    logging.info(f"Saved to {EXPLOITDB_CSV_LOCAL}")


def process_exploitdb_csv():
    import re
    logging.info("Processing exploits, tags, and metadata and writing to CSVs ...")
    processed_csv = os.path.join(EXPLOITDB_DATA_DIR, "processed_exploits.csv")
    tags_csv = os.path.join(EXPLOITDB_DATA_DIR, "exploit_tags.csv")
    metadata_csv = os.path.join(EXPLOITDB_DATA_DIR, "exploit_metadata.csv")
    with open(EXPLOITDB_CSV_LOCAL, newline='', encoding='utf-8') as infile, \
         open(processed_csv, 'w', newline='', encoding='utf-8') as exploits_out, \
         open(tags_csv, 'w', newline='', encoding='utf-8') as tags_out, \
         open(metadata_csv, 'w', newline='', encoding='utf-8') as meta_out:
        reader = csv.DictReader(infile)
        exploits_fields = [
            'cve_id', 'source', 'source_id', 'url', 'title', 'description', 'date_published', 'technique'
        ]
        tags_fields = ['exploit_id', 'tag']
        meta_fields = ['exploit_id', 'meta_key', 'meta_value']
        exploits_writer = csv.DictWriter(exploits_out, fieldnames=exploits_fields)
        tags_writer = csv.DictWriter(tags_out, fieldnames=tags_fields)
        meta_writer = csv.DictWriter(meta_out, fieldnames=meta_fields)
        exploits_writer.writeheader()
        tags_writer.writeheader()
        meta_writer.writeheader()
        count = 0
        tag_count = 0
        meta_count = 0
        for row in reader:
            cve_ids = set()
            if row['description']:
                cve_ids = set(filter(None, re.findall(r'CVE-\d{4}-\d{4,}', row['description'])))
            if not cve_ids:
                cve_ids = {f"NOCVE-{row['id']}"}
            for cve_id in cve_ids:
                exploits_writer.writerow({
                    'cve_id': cve_id,
                    'source': 'source_edb',
                    'source_id': row['id'],
                    'url': f"https://www.exploit-db.com/exploits/{row['id']}",
                    'title': row['description'][:100] if row['description'] else '',
                    'description': row['description'] or '',
                    'date_published': row['date_published'],
                    'technique': row['type']
                })
                count += 1
            # Extract tags (if present)
            if 'tags' in row and row['tags']:
                for tag in row['tags'].split(';'):
                    if tag.strip():
                        tags_writer.writerow({'exploit_id': row['id'], 'tag': tag.strip()})
                        tag_count += 1
            # Extract metadata (all columns except id, description, date_published, type, tags)
            for key in row:
                if key not in ['id', 'description', 'date_published', 'type', 'tags'] and row[key]:
                    meta_writer.writerow({'exploit_id': row['id'], 'meta_key': key, 'meta_value': row[key]})
                    meta_count += 1
    logging.info(f"Wrote {count} exploit rows to {processed_csv}")
    logging.info(f"Wrote {tag_count} tag rows to {tags_csv}")
    logging.info(f"Wrote {meta_count} metadata rows to {metadata_csv}")
    return processed_csv, tags_csv, metadata_csv


def import_to_postgres(processed_csv, tags_csv, metadata_csv, dry_run=False, metrics=None):
    # Ensure all CVEs in processed_csv are present in canonical table
    import csv
    with open(processed_csv, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        conn = psycopg2.connect(**PG_CONFIG)
        try:
            with conn:
                for row in reader:
                    ensure_cve_exists(conn, row.get('cve_id'), source='exploitdb')
        finally:
            conn.close()

    logging.info("Importing exploits, tags, and metadata into PostgreSQL with COPY ...")
    conn = None
    if metrics is None:
        metrics = RunMetrics(dry_run=dry_run)
    try:
        conn = psycopg2.connect(**PG_CONFIG)
        cursor = conn.cursor()
        # Exploits table
        safe_execute(cursor,
"""
CREATE TABLE IF NOT EXISTS exploits (
    cve_id TEXT,
    source TEXT,
    source_id TEXT,
    url TEXT,
    title TEXT,
    description TEXT,
    date_published TEXT,
    technique TEXT,
    imported_at TIMESTAMP DEFAULT now(),
    UNIQUE(cve_id, source_id)
);
""",
        dry_run=dry_run)
        # Ensure unique constraint exists (for legacy DBs)
        try:
            safe_execute(cursor,
                "ALTER TABLE exploits ADD CONSTRAINT exploits_cve_id_source_id_unique UNIQUE (cve_id, source_id);",
                dry_run=dry_run)
        except Exception as e:
            if 'already exists' in str(e):
                logging.info("Unique constraint on (cve_id, source_id) already exists.")
            else:
                logging.warning(f"Could not add unique constraint to exploits: {e}")
        # Upsert exploits row-by-row
        with open(processed_csv, 'r', encoding='utf-8') as f:
            import csv
            reader = csv.DictReader(f)
            batch = []
            batch_size = 1000
            count = 0
            for row in reader:
                batch.append(row)
                if len(batch) >= batch_size:
                    safe_executemany(cursor,
                        "INSERT INTO exploits (cve_id, source, source_id, url, title, description, date_published, technique, imported_at) VALUES (%(cve_id)s, %(source)s, %(source_id)s, %(url)s, %(title)s, %(description)s, %(date_published)s, %(technique)s, now()) "
                        "ON CONFLICT (cve_id, source_id) DO UPDATE SET "
                        "description = EXCLUDED.description, "
                        "imported_at = CASE WHEN exploits.description IS DISTINCT FROM EXCLUDED.description THEN now() ELSE exploits.imported_at END;",
                        batch,
                        dry_run=dry_run
                    )
                    count += len(batch)
                    metrics.inserts += len(batch)
                    logging.info(f"Upserted {count} exploits so far...")
                    batch = []
            if batch:
                safe_executemany(cursor,
                    "INSERT INTO exploits (cve_id, source, source_id, url, title, description, date_published, technique, imported_at) VALUES (%(cve_id)s, %(source)s, %(source_id)s, %(url)s, %(title)s, %(description)s, %(date_published)s, %(technique)s, now()) "
                    "ON CONFLICT (cve_id, source_id) DO UPDATE SET "
                    "description = EXCLUDED.description, "
                    "imported_at = CASE WHEN exploits.description IS DISTINCT FROM EXCLUDED.description THEN now() ELSE exploits.imported_at END;",
                    batch,
                    dry_run=dry_run
                )
                count += len(batch)
                metrics.inserts += len(batch)
                logging.info(f"Upserted {count} exploits in total.")
        logging.info("Upserted exploits into PostgreSQL.")
        # Tags table
        cursor.execute(
"""
CREATE TABLE IF NOT EXISTS exploit_tags (
    exploit_id TEXT,
    tag TEXT,
    imported_at TIMESTAMP DEFAULT now(),
    UNIQUE(exploit_id, tag)
);
""")
        # Upsert tags row-by-row
        with open(tags_csv, 'r', encoding='utf-8') as f:
            import csv
            reader = csv.DictReader(f)
            batch = []
            count = 0
            for row in reader:
                batch.append(row)
                if len(batch) >= batch_size:
                    cursor.executemany(
                        "INSERT INTO exploit_tags (exploit_id, tag, imported_at) VALUES (%(exploit_id)s, %(tag)s, now()) "
                        "ON CONFLICT (exploit_id, tag) DO UPDATE SET "
                        "imported_at = CASE WHEN exploit_tags.tag IS DISTINCT FROM EXCLUDED.tag THEN now() ELSE exploit_tags.imported_at END;",
                        batch
                    )
                    count += len(batch)
                    logging.info(f"Upserted {count} exploit tags so far...")
                    batch = []
            if batch:
                cursor.executemany(
                    "INSERT INTO exploit_tags (exploit_id, tag, imported_at) VALUES (%(exploit_id)s, %(tag)s, now()) "
                    "ON CONFLICT (exploit_id, tag) DO UPDATE SET "
                    "imported_at = CASE WHEN exploit_tags.tag IS DISTINCT FROM EXCLUDED.tag THEN now() ELSE exploit_tags.imported_at END;",
                    batch
                )
                count += len(batch)
                logging.info(f"Upserted {count} exploit tags in total.")
        logging.info("Upserted exploit tags into PostgreSQL.")
        # Metadata table
        cursor.execute(
"""
CREATE TABLE IF NOT EXISTS exploit_metadata (
    exploit_id TEXT,
    meta_key TEXT,
    meta_value TEXT,
    imported_at TIMESTAMP DEFAULT now(),
    UNIQUE(exploit_id, meta_key)
);
""")
        # Upsert metadata row-by-row
        with open(metadata_csv, 'r', encoding='utf-8') as f:
            import csv
            reader = csv.DictReader(f)
            batch = []
            count = 0
            for row in reader:
                batch.append(row)
                if len(batch) >= batch_size:
                    cursor.executemany(
                        "INSERT INTO exploit_metadata (exploit_id, meta_key, meta_value, imported_at) VALUES (%(exploit_id)s, %(meta_key)s, %(meta_value)s, now()) "
                        "ON CONFLICT (exploit_id, meta_key) DO UPDATE SET "
                        "meta_value = EXCLUDED.meta_value, "
                        "imported_at = CASE WHEN exploit_metadata.meta_value IS DISTINCT FROM EXCLUDED.meta_value THEN now() ELSE exploit_metadata.imported_at END;",
                        batch
                    )
                    count += len(batch)
                    logging.info(f"Upserted {count} exploit metadata rows so far...")
                    batch = []
            if batch:
                cursor.executemany(
                    "INSERT INTO exploit_metadata (exploit_id, meta_key, meta_value, imported_at) VALUES (%(exploit_id)s, %(meta_key)s, %(meta_value)s, now()) "
                    "ON CONFLICT (exploit_id, meta_key) DO UPDATE SET "
                    "meta_value = EXCLUDED.meta_value, "
                    "imported_at = CASE WHEN exploit_metadata.meta_value IS DISTINCT FROM EXCLUDED.meta_value THEN now() ELSE exploit_metadata.imported_at END;",
                    batch
                )
                count += len(batch)
                logging.info(f"Upserted {count} exploit metadata rows in total.")
        logging.info("Upserted exploit metadata into PostgreSQL.")
    except Exception as e:
        if ENV == 'production':
            logging.error("PostgreSQL Error occurred.")
        else:
            logging.error(f"PostgreSQL Error: {e}", exc_info=True)
        raise
    finally:
        if conn:
            cursor.close()
            conn.close()


def main():
    import argparse
    from etl_utils import RunMetrics, safe_execute, safe_executemany, dry_run_notice
    parser = argparse.ArgumentParser(description="ExploitDB ETL")
    parser.add_argument('--dry-run', '-n', action='store_true', help='Run ETL without writing to the database')
    args = parser.parse_args()
    metrics = RunMetrics(dry_run=args.dry_run)
    if args.dry_run:
        dry_run_notice()
    download_exploitdb_csv()
    processed_csv, tags_csv, metadata_csv = process_exploitdb_csv()
    metrics.fetched = sum(1 for _ in open(processed_csv, encoding='utf-8')) - 1  # minus header
    import_to_postgres(processed_csv, tags_csv, metadata_csv, dry_run=args.dry_run, metrics=metrics)
    metrics.log_summary()
    for file in [EXPLOITDB_CSV_LOCAL, processed_csv, tags_csv, metadata_csv]:
        try:
            os.remove(file)
        except Exception as e:
            logging.warning(f"Could not remove temp file {file}: {e}")
    logging.info("\n=== ExploitDB Full Import Completed (Python) ===\n")


if __name__ == "__main__":
    main()
