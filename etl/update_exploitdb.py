import os
import csv
import requests
import psycopg2
from io import StringIO
import logging

# Set up logging based on environment
ENV = os.environ.get('ENV', 'development').lower()
LOG_LEVEL = logging.INFO if ENV == 'production' else logging.DEBUG
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')

EXPLOITDB_DATA_DIR = os.path.join(os.path.dirname(__file__), 'exploitdb-data')
if not os.path.exists(EXPLOITDB_DATA_DIR):
    os.makedirs(EXPLOITDB_DATA_DIR)
EXPLOITDB_CSV_LOCAL = os.path.join(EXPLOITDB_DATA_DIR, "files_exploits.csv")
EXPLOITS_CSV = os.path.join(EXPLOITDB_DATA_DIR, "exploits.csv")
def read_secret(secret_path, default):
    try:
        with open(secret_path, 'r') as f:
            return f.read().strip()
    except Exception:
        return default

PG_CONFIG = {
    'host': os.environ.get('PGHOST', 'db'),
    'user': read_secret('/run/secrets/pg_user', os.environ.get('PGUSER', 'postgres')),
    'password': read_secret('/run/secrets/pg_password', os.environ.get('PGPASSWORD', 'postgres')),
    'dbname': os.environ.get('PGDATABASE', 'epssdb'),
}

EXPLOITDB_CSV_URL = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"


def download_exploitdb_csv():
    logging.info(f"Downloading ExploitDB CSV from {EXPLOITDB_CSV_URL} ...")
    resp = requests.get(EXPLOITDB_CSV_URL)
    resp.raise_for_status()
    with open(EXPLOITDB_CSV_LOCAL, "w", encoding="utf-8") as f:
        f.write(resp.text)
    logging.info(f"Saved to {EXPLOITDB_CSV_LOCAL}")


def process_exploitdb_csv():
    import re
    logging.info("Processing exploits, tags, and metadata and writing to CSVs ...")
    processed_csv = os.path.join(EXPLOITDB_DATA_DIR, "processed_exploits.csv")
    tags_csv = os.path.join(EXPLOITDB_DATA_DIR, "exploit_tags.csv")
    metadata_csv = os.path.join(EXPLOITDB_DATA_DIR, "exploit_metadata.csv")
    with open(EXPLOITDB_CSV_LOCAL, newline='', encoding='utf-8') as infile, \
         open(processed_csv, 'w', newline='', encoding='utf-8') as exploits_out, \
         open(tags_csv, 'w', newline='', encoding='utf-8') as tags_out, \
         open(metadata_csv, 'w', newline='', encoding='utf-8') as meta_out:
        reader = csv.DictReader(infile)
        exploits_fields = [
            'cve_id', 'source', 'source_id', 'url', 'title', 'description', 'date_published', 'technique'
        ]
        tags_fields = ['exploit_id', 'tag']
        meta_fields = ['exploit_id', 'meta_key', 'meta_value']
        exploits_writer = csv.DictWriter(exploits_out, fieldnames=exploits_fields)
        tags_writer = csv.DictWriter(tags_out, fieldnames=tags_fields)
        meta_writer = csv.DictWriter(meta_out, fieldnames=meta_fields)
        exploits_writer.writeheader()
        tags_writer.writeheader()
        meta_writer.writeheader()
        count = 0
        tag_count = 0
        meta_count = 0
        for row in reader:
            cve_ids = set()
            if row['description']:
                cve_ids = set(filter(None, re.findall(r'CVE-\d{4}-\d{4,}', row['description'])))
            if not cve_ids:
                cve_ids = {f"NOCVE-{row['id']}"}
            for cve_id in cve_ids:
                exploits_writer.writerow({
                    'cve_id': cve_id,
                    'source': 'source_edb',
                    'source_id': row['id'],
                    'url': f"https://www.exploit-db.com/exploits/{row['id']}",
                    'title': row['description'][:100] if row['description'] else '',
                    'description': row['description'] or '',
                    'date_published': row['date_published'],
                    'technique': row['type']
                })
                count += 1
            # Extract tags (if present)
            if 'tags' in row and row['tags']:
                for tag in row['tags'].split(';'):
                    if tag.strip():
                        tags_writer.writerow({'exploit_id': row['id'], 'tag': tag.strip()})
                        tag_count += 1
            # Extract metadata (all columns except id, description, date_published, type, tags)
            for key in row:
                if key not in ['id', 'description', 'date_published', 'type', 'tags'] and row[key]:
                    meta_writer.writerow({'exploit_id': row['id'], 'meta_key': key, 'meta_value': row[key]})
                    meta_count += 1
    logging.info(f"Wrote {count} exploit rows to {processed_csv}")
    logging.info(f"Wrote {tag_count} tag rows to {tags_csv}")
    logging.info(f"Wrote {meta_count} metadata rows to {metadata_csv}")
    return processed_csv, tags_csv, metadata_csv


def import_to_postgres(processed_csv, tags_csv, metadata_csv):
    logging.info("Importing exploits, tags, and metadata into PostgreSQL with COPY ...")
    conn = None
    try:
        conn = psycopg2.connect(**PG_CONFIG)
        conn.autocommit = True
        cursor = conn.cursor()
        # Exploits table
        cursor.execute(
"""
CREATE TABLE IF NOT EXISTS exploits (
    cve_id TEXT,
    source TEXT,
    source_id TEXT UNIQUE,
    url TEXT,
    title TEXT,
    description TEXT,
    date_published TEXT,
    technique TEXT
);
""")
        batch_size = 10000
        # Upsert exploits row-by-row
        with open(processed_csv, 'r', encoding='utf-8') as f:
            import csv
            reader = csv.DictReader(f)
            batch = []
            count = 0
            for row in reader:
                batch.append(row)
                if len(batch) >= batch_size:
                    cursor.executemany(
                        "INSERT INTO exploits (cve_id, source, source_id, url, title, description, date_published, technique) "
                        "VALUES (%(cve_id)s, %(source)s, %(source_id)s, %(url)s, %(title)s, %(description)s, %(date_published)s, %(technique)s) "
                        "ON CONFLICT (source_id) DO UPDATE SET cve_id = EXCLUDED.cve_id, source = EXCLUDED.source, url = EXCLUDED.url, title = EXCLUDED.title, description = EXCLUDED.description, date_published = EXCLUDED.date_published, technique = EXCLUDED.technique;",
                        batch
                    )
                    count += len(batch)
                    logging.info(f"Upserted {count} exploits so far...")
                    batch = []
            if batch:
                cursor.executemany(
                    "INSERT INTO exploits (cve_id, source, source_id, url, title, description, date_published, technique) "
                    "VALUES (%(cve_id)s, %(source)s, %(source_id)s, %(url)s, %(title)s, %(description)s, %(date_published)s, %(technique)s) "
                    "ON CONFLICT (source_id) DO UPDATE SET cve_id = EXCLUDED.cve_id, source = EXCLUDED.source, url = EXCLUDED.url, title = EXCLUDED.title, description = EXCLUDED.description, date_published = EXCLUDED.date_published, technique = EXCLUDED.technique;",
                    batch
                )
                count += len(batch)
                logging.info(f"Upserted {count} exploits in total.")
        logging.info("Upserted exploits into PostgreSQL.")
        # Tags table
        cursor.execute(
"""
CREATE TABLE IF NOT EXISTS exploit_tags (
    exploit_id TEXT,
    tag TEXT,
    UNIQUE(exploit_id, tag)
);
""")
        # Upsert tags row-by-row
        with open(tags_csv, 'r', encoding='utf-8') as f:
            import csv
            reader = csv.DictReader(f)
            batch = []
            count = 0
            for row in reader:
                batch.append(row)
                if len(batch) >= batch_size:
                    cursor.executemany(
                        "INSERT INTO exploit_tags (exploit_id, tag) VALUES (%(exploit_id)s, %(tag)s) ON CONFLICT (exploit_id, tag) DO NOTHING;",
                        batch
                    )
                    count += len(batch)
                    logging.info(f"Upserted {count} exploit tags so far...")
                    batch = []
            if batch:
                cursor.executemany(
                    "INSERT INTO exploit_tags (exploit_id, tag) VALUES (%(exploit_id)s, %(tag)s) ON CONFLICT (exploit_id, tag) DO NOTHING;",
                    batch
                )
                count += len(batch)
                logging.info(f"Upserted {count} exploit tags in total.")
        logging.info("Upserted exploit tags into PostgreSQL.")
        # Metadata table
        cursor.execute(
"""
CREATE TABLE IF NOT EXISTS exploit_metadata (
    exploit_id TEXT,
    meta_key TEXT,
    meta_value TEXT,
    UNIQUE(exploit_id, meta_key)
);
""")
        # Upsert metadata row-by-row
        with open(metadata_csv, 'r', encoding='utf-8') as f:
            import csv
            reader = csv.DictReader(f)
            batch = []
            count = 0
            for row in reader:
                batch.append(row)
                if len(batch) >= batch_size:
                    cursor.executemany(
                        "INSERT INTO exploit_metadata (exploit_id, meta_key, meta_value) VALUES (%(exploit_id)s, %(meta_key)s, %(meta_value)s) ON CONFLICT (exploit_id, meta_key) DO UPDATE SET meta_value = EXCLUDED.meta_value;",
                        batch
                    )
                    count += len(batch)
                    logging.info(f"Upserted {count} exploit metadata rows so far...")
                    batch = []
            if batch:
                cursor.executemany(
                    "INSERT INTO exploit_metadata (exploit_id, meta_key, meta_value) VALUES (%(exploit_id)s, %(meta_key)s, %(meta_value)s) ON CONFLICT (exploit_id, meta_key) DO UPDATE SET meta_value = EXCLUDED.meta_value;",
                    batch
                )
                count += len(batch)
                logging.info(f"Upserted {count} exploit metadata rows in total.")
        logging.info("Upserted exploit metadata into PostgreSQL.")
    except Exception as e:
        if ENV == 'production':
            logging.error("PostgreSQL Error occurred.")
        else:
            logging.error(f"PostgreSQL Error: {e}", exc_info=True)
        raise
    finally:
        if conn:
            cursor.close()
            conn.close()


def main():
    download_exploitdb_csv()
    processed_csv, tags_csv, metadata_csv = process_exploitdb_csv()
    import_to_postgres(processed_csv, tags_csv, metadata_csv)
    for file in [EXPLOITDB_CSV_LOCAL, processed_csv, tags_csv, metadata_csv]:
        os.remove(file)
    logging.info("\n=== ExploitDB Full Import Completed (Python) ===\n")


if __name__ == "__main__":
    main()
